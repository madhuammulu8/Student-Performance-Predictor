# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FLUmNU3-RmT1s7KJ--Hz1NnQAQwUsvMF
"""

import pandas as pd
from sklearn import preprocessing
import numpy as np

df = pd.read_csv("student_data.csv")
df["passed"] = df["passed"].astype('category')
df["passed"] = df["passed"].cat.codes
outcomes = df['passed']
data = df.drop('passed', axis = 1)
data = data.dropna()
data = data.apply(lambda col: pd.factorize(col, sort=True)[0])
scaler = preprocessing.MinMaxScaler()
scaler.fit(data)
scaled = scaler.transform(data)
train = pd.DataFrame(scaled, columns=data.columns)

#Decision Tree Scratch
import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

class Node:

    def __init__(self, features=None, maxload=None, down=None, up=None, *, vals=None):
        self.features = features
        self.maxload = maxload
        self.down = down
        self.up = up
        self.vals = vals

    def leafApex(self):
        return self.vals is not None

class DecisionTrees:

    def __init__(self, plinthRepSplit=2, supreme_depth=100, nAchv=None):
        self.plinthRepSplit = plinthRepSplit
        self.supreme_depth = supreme_depth
        self.nAchv = nAchv
        self.origin = None

    def forecast(self, S):
        return np.array([self.treeTraversal(s, self.origin) for s in S])

    def treeExplore(self, S, T, depth=0):
        rep, features = S.shape

        if depth >= self.supreme_depth:
            leafVal = Counter(T).most_common(1)[0][0]
            return Node(vals=leafVal)
        elif len(np.unique(T)) == 1:
            leafVal = Counter(T).most_common(1)[0][0]
            return Node(vals=leafVal)
        elif rep < self.plinthRepSplit:
            leafVal = Counter(T).most_common(1)[0][0]
            return Node(vals=leafVal)

        bagIds = np.random.choice(features, self.nAchv, replace=False)

        bestResult, bestMaxload = self.bestCriteria(S, T, bagIds)

        downIds = np.argwhere(S[:, bestResult] <= bestMaxload).flatten()
        upId = np.argwhere(S[:, bestResult] > bestMaxload).flatten()
        upward = self.treeExplore(S[upId, :], T[upId], depth+1)
        downward = self.treeExplore(S[downIds, :], T[downIds], depth+1)
        return Node(bestResult, bestMaxload, downward, upward)

    def toughness(self, S, T):
        self.nAchv = S.shape[1] if not self.nAchv else min(self.nAchv, S.shape[1])
        self.origin = self.treeExplore(S, T)

    def bestCriteria(self, S, T, bagIds):
        bestGain = -1
        splitId, splitMaxload = None, None
        for bag in bagIds:
            SCol = S[:, bag]
            maxLoads = np.unique(SCol)
            for maxld in maxLoads:
                gain = self.infoGain(T, SCol, maxld)
                if gain > bestGain:
                    bestGain = gain
                    splitId = bag
                    splitMaxload = maxld

        return splitId, splitMaxload

    def infoGain(self, T, SCol, splitMaxload):

        pEntropies = -np.sum([p * np.log2(p) for p in (np.bincount(T) / len(T)) if p > 0])
        downwardIds = np.argwhere(SCol <= splitMaxload).flatten()
        upwardIds = np.argwhere(SCol > splitMaxload).flatten()
        if len(downwardIds) == 0:
            return 0
        elif len(upwardIds) == 0:
            return 0
        e_downward = -np.sum([p * np.log2(p) for p in (np.bincount(T[downwardIds]) / len(T[downwardIds])) if p > 0])
        eUp = -np.sum([p * np.log2(p) for p in (np.bincount(T[upwardIds]) / len(T[upwardIds])) if p > 0])
        cEntropies = (len(downwardIds) / len(T)) * e_downward + (len(upwardIds) / len(T)) * eUp
        return pEntropies - cEntropies

    def treeTraversal(self, R, nodes):
        if nodes.leafApex():
            return nodes.vals

        if R[nodes.features] <= nodes.maxload:
            return self.treeTraversal(R, nodes.down)
        elif R[nodes.features] > nodes.maxload:
            return self.treeTraversal(R, nodes.up)

from sklearn.model_selection import train_test_split
def accuracy(YReal, YPredict):
    x = np.sum(YReal == YPredict)
    y = len(YReal) 
    accuracy = x/y
    return accuracy

print("Desision Tree Algorithm")

STrain, STest, TTrain, TTest = train_test_split(train.values, outcomes.values, train_size=0.8, random_state=2)

classification=DecisionTrees(supreme_depth=10)
classification.toughness(STrain,TTrain)

TPredict1 = classification.forecast(STrain)
acc1 = accuracy(TTrain, TPredict1)
print("\nTraining Accuracy results : ", acc1)

TPredict2 = classification.forecast(STest) 
acc2 = accuracy(TTest, TPredict2)
print("\nTesting Accuracy results : ", acc2)

print("\nTrain size: 0.8")

from sklearn import metrics
print("\nConfusion Matrix:")
print(metrics.confusion_matrix(TTest, TPredict2, labels=[1,2,3,4,5,6,7,8,9]))
print("\nClassification Report:")
print(metrics.classification_report(TTest, TPredict2, labels=[1,2,3,4,5,6,7,8,9]))